[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is just a place for me to put notes or writeups of any projects I’ve been working on. I mostly use R, and I try to make all the stuff on here reproducible - so you should (hopefully!) be able to run everything here yourself."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Figuring out samples sizes and detectable effects using simulation\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nDo museums cause antisocial behaviour? No. If a model says museums do cause antisocial behaviour, should we trust it? Also no.\n\n\n\n\n\n\nJul 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nA slightly math-y look at directly & indirectly standardised rates. Talks about how all rates are just weighted sums, and the link between DSRs and poisson regression\n\n\n\n\n\n\nMar 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\nAn introduction to random forests, with pictures instead of maths / code\n\n\n\n\n\n\nMay 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dags_museums_asbos/index.html",
    "href": "posts/dags_museums_asbos/index.html",
    "title": "Museums, antisocial behaviour, and DAGs",
    "section": "",
    "text": "A few months ago I went on a two day “causal inference workshop”, ran by the fabulous Economics Professor Scott Cunningham. I have zero background in economics, but Scott is a great teacher and I learnt a lot from the workshop (even if it did feel like drinking from a firehose at times!). One thing which really stuck with me is Scott’s use of DAGs - Directed Acyclic Graphs - and how they can be used to help clarify the “mental model” you use when you’re trying to do inference.\nIt’s easiest to show how DAGs work by doing examples. Before jumping in, I’d like to plug a book Scott is working on - Causal Inference: The Mixtape. The book covers everything in this post & much more, so if you find this sort of stuff interesting, go read the mixtape!\n\nMuseums and antisocial behaviour\nIn the UK, most government organisations are really good at releasing data to the public. Two examples are\n\nThe police collect monthly stats on police activity - available here if you’re interested\nThe Department for Culture, Media, and Sport collect monthly visits to museums & galleries - avaliable here if you’re interested\n\nCombining these together gives us one dataset with monthly figures for the number of musueum visits & the number of “antisocial behaviour” crimes responded to by police officers. We can run a regression:\n\nmod1 <- lm(antisocial ~ visits, data = df)\nsummary(mod1)\n\nCall:\nlm(formula = antisocial ~ visits, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24000 -15395  -7449  12726  37095 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 7.002e+04  2.301e+04   3.043  0.00474 **\nvisits      1.380e-02  5.549e-03   2.488  0.01845 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 18610 on 31 degrees of freedom\nMultiple R-squared:  0.1664,    Adjusted R-squared:  0.1395 \nF-statistic: 6.188 on 1 and 31 DF,  p-value: 0.01845\nThere is a statistically significant relationship between museum visits and antisocial behaviour, and we’re able to say things like “increasing monthly museum visits by 1000 people will cause an extra 14 antisocial crimes”.\nBut do you believe this? Does it seem likely that museum visits causes antisocial behaviour? Should we ban museums? Should we stop school kids & tourists from visiting museums?\n\n\nWhats going on? A mental model\nIt’s important to realise what you’re doing when you run\n\nmod1 <- lm(antisocial ~ visits, data = df)\n\nWhen you run this you are trying to fit a model, antisocial ~ visits, to some data, df. That model has some assumptions which you need to be aware of. In particular, you assume that\n\nthere is a relationship between antisocial behaviour and museum visits, and\nthat the relationship is linear, so it looks like this: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\nIf either of these assumptions aren’t true, you can end up with some pretty strange conclusions.\nDAGs are really handy ways of writing down your assumptions. Here’s the DAG we assume when we use the model antisocial ~ visits:\n\n\n\n\n\nHere’s the rules for drawing DAGs:\n\nWrite down your independent variable, and all the variables you think matter\nDraw arrows between the variables if you believe there is a causal link between them\n\nSo for our mental model, we are assuming that the only thing what has a causal effect on antisocial behaviour is museum visits.\n\n\nA better mental model\nNow that we’ve explicity stated our mental model, it doesn’t sound very realistic. Here’s another potential model:\n\n\n\n\n\nWhenever you see a DAG, try to say in words what it’s telling you. This one is saying that there is a causal link between temperature and museum visits, and a causal link between temperature and antisocial behaviour. This model seems a bit more reasonable. There will be more people walking around on hot days. This gives more opportunities for people to visit a museum (e.g. someone walks past a museum and wanders in), and also more opportunities for antisocial behaviour (since there’s more potential victims).\nNotice that there isn’t an arrow between museum visits and antisocial behaviour in our new model. This is another assumption we’re making. We are assuming that museum visits has no causal effect on antisocial behaviour. Given this assumption, it doesn’t really make sense to try to investigate the effect of museum visits on antisocial behaviour - there won’t be any!\nLet’s see what happens when we add temperature to the model:\n\nmod2 <- lm(antisocial ~ visits + avg_temp, data = df)\nsummary(mod2)\n\nCall:\nlm(formula = antisocial ~ visits + avg_temp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28300 -12112  -2595   8564  32713 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 9.617e+04  2.181e+04   4.408 0.000123 ***\nvisits      9.509e-04  6.326e-03   0.150 0.881525    \navg_temp    2.571e+03  8.065e+02   3.188 0.003343 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 16350 on 30 degrees of freedom\nMultiple R-squared:  0.3773,    Adjusted R-squared:  0.3358 \nF-statistic: 9.089 on 2 and 30 DF,  p-value: 0.0008204\nThere’s no relationship between museum visits and antisocial behaviour - exactly as we said!\n\n\nDAG theory 101\nThere is a lot of stuff going on behind the scenes with DAGs. If you want to learn about DAGs in detail, check out any book by Judea Pearl or this great course by Miguel Hernan.\nHere’s all the information you need to get started using DAGs. You need to know about backdoor paths, colliders and confounders.\n\n\n\n\n\nIf there is an arrow connecting two nodes together, we say that there is a direct path between the nodes. In the DAGs above, there are the following direct paths\n\nA \\(\\to\\) B\nA \\(\\to\\) C\nY \\(\\to\\) X\nZ \\(\\to\\) X\n\nNotice that there is no direct path between B & C, but there is a way you can get from B to C (by going B \\(\\to\\) A \\(\\to\\) C). This indirect path is called a backdoor path. Backdoors can introduce bias in your model, so you need to make sure that you “shut” all the backdoors during the model stage.\nThere are two types of nodes on backdoor paths, colliders and confounders. Node A is an example of a confounder, since it has multiple arrows going out of it. Node X is an example of a collider, since there are multiple arrows “colliding” together at X.\nYou need to keep track of the different types of nodes along a backdoor path. Depending on what type of node is on the path, there are two ways to deal with backdoors:\n\nIf the path contains one or more confounders, add one of the confounders to the model. You don’t need to add every confounder along the path into your model, just one\nIf the path contains one or more colliders, do nothing. Do not add the variables on the backdoor path to the model\n\nArmed with a bit of theory, let’s have another look at the models we fit earlier. Here’s our mental model for museum visits and antisocial bahaviour:\n\n\n\n\n\nAnd here’s the two models we fit:\n\nmod1 <- lm(antisocial ~ visits, data = df)\nmod2 <- lm(antisocial ~ visits + avg_temp, data = df)\n\nTemperature is a confounder in our DAG. In mod1 we don’t control for the counfounder, which introduces bias to the model. This bias made us incorrectly conclude that there was a significant relationship between museum visits and antisocial behaviour. In mod2 we do control for the counfounder. This model is correctly specified, so there is no bias and we correctly conclude that there is no link between museum visits and antisocial behaviour.\nThis is the benefit of DAGs. By forcing you to write down all the assumed relationships before you start building models, you get a much better understanding of what variables you should include / exclude from the model.\n\n\nThe gender pay gap\nThis example is lifted from chapter 3 of Causal Inference: The Mixtape. The example originally comes from University of Liverpool Economist Erin Hengel. The only change I have made is converting the code from Stata to R.\nThere has been a lot of interest on the gender pay gap in recent years. If you’re not familiar, the gender pay gap is the gap between salary for men & women - women tend to be paid less than men. The most common argument against the pay gap is that, once you control for job role, the gender pay gap disappears.\nBut is controlling for job role a valid thing to do? From the previous section, we saw that controlling on colliders will introduce bias to a model and potentially lead you to the wrong conclusion. Drawing a DAG will help us decide if we should control for job role.\nHere’s one possible model:\n\n\n\n\n\nThere’s a lot going on here, so break it down:\n\nOnly females receive discrimination\nThe occupation someone works in is affected by three things - any discrimination experienced, the persons intrinsic ability to do the job, and if the person is female\nWages are affected by the job the person works in, the persons intrinsic ability, and the level of discrimination the person receives\n\nThese feel like reasonable assumptions - I wouldn’t want to work in a job where I’m discriminated or if I don’t have the ability to succeed at the job. I’d also be less likely to apply for a job if there was no-one like me working there. This is why there is an arrow between F \\(\\to\\) O. It’s saying that women are less likely to apply for jobs in male-dominated fields. The wage assumptions also make sense to me. I would expect a bus driver to be paid less than an astronaut. If someone is brilliant at their job, the company is more likely to pay them a higher salary so they keep working at the company.\nThere’s a fair few paths between D and W in this DAG, which will make listing all the backdoor paths tedious. Fortunately there is the dagitty package which does all the hard work for you. All you need to do is\n\nSpecify all the nodes in the DAG. This is the F;D;W;O;A line in the code below\nNext you start drawing all your arrows between the nodes. This is the F -> O lines in the code below\n\n\nlibrary(dagitty)\n\ngender <- dagitty('dag {\n             F;D;W;O;A\n                 \n             F -> O\n             F -> D\n             D -> O\n             D -> W\n             O -> W\n             A -> O\n             A -> W\n}')\n\nNow you can figure out all the paths between D and W by calling the paths function:\n\np <- paths(gender, from = 'D', to = 'W')\n\np has two elements - $paths which lists all the paths between D and W, and $open which says if each path is open or closed. Here’s all the paths:\n\np$paths\n\n[1] \"D -> O -> W\"           \"D -> O <- A -> W\"     \n[3] \"D -> W\"                \"D <- F -> O -> W\"     \n[5] \"D <- F -> O <- A -> W\"\nThere’s the direct path D -> W which we’re interested in, and 4 backdoor paths which might bias our model. Let’s have a look at which paths are open:\n\np$paths[p$open]\n\n[1] \"D -> O -> W\"      \"D -> W\"           \"D <- F -> O -> W\"\nAnd which paths are closed:\n\np$paths[!p$open]\n\n[1] \"D -> O <- A -> W\"      \"D <- F -> O <- A -> W\"\nWe need to close the open paths D -> O -> W and D <- F -> O -> W. The easiest way to do this is to control on occupation O. But O is a collider on the closed paths D -> O <- A -> W and D <- F -> O <- A -> W. Including O in the model will close the two open paths, but also open the two closed paths! We need to add another control in to close these newly opened backdoor paths, and we can do that by also controlling on A.\nThis tells us the correct model specification to use. We should run wage ~ discrimination + occupation + ability. Anything else will be biased and wrong.\n\n\nA simulation\nUnfortunately we can’t measure ability, since ability is unobserved. There isn’t a spreadsheet somewhere which scores how “able” someone is to perform their job.\nAs a check to see if wage ~ discrimination + occupation + ability is the correct model, we’re going to do a simulation. Simulations are really handy ways to check if we are fitting the right sort of model. We first simulate (make up) some data, where the independent variable is distributed with some parameters which we set. We then fit a model to this data, and see how close the model parameters are to the true parameters. If we have specified the model correctly, the model parameters should match up with the actual parameters.\n\nn <- 10000 # Simulate 10,000 observations\n\nfemale <- runif(n) > 0.5 # Set 50% of the observations to female\nability <- rnorm(n) # Ability is normally distributed\ndiscrimination <- female # All women experience discrimination\n\n# Occupation depends on ability, discrimination, and if \n# the person is female\noccupation <- 1 + 2 * ability -2 * discrimination + 0.5 * female + rnorm(n)\n\n# Wage depends on discrimination, occupation, and ability\nwage <- 1 -1 * discrimination + 1 * occupation + 2 * ability + rnorm(n)\n\nIf our model is correct, we should get a coefficient of -1 on the discrimination term, a coefficient of +1 on the occupation term, and +2 on the ability term. Let’s fit 3 models:\n\nmod1 <- lm(wage ~ discrimination)\nmod2 <- lm(wage ~ discrimination + occupation)\nmod3 <- lm(wage ~ discrimination + occupation + ability)\n\nThe texreg package makes a nice regression table we can use to compare the models. It shows the coeffecient estimates for each model along with their standard errors (the number in brackets). It also puts little stars next to the estimates to show if a parameter estimate is significant or not:\n\ntexreg::screenreg(list(mod1, mod2, mod3))\n\n============================================================\n                Model 1       Model 2       Model 3     \n------------------------------------------------------------\n(Intercept)             2.03 ***      0.20 ***      1.03 ***\n                       (0.06)        (0.02)        (0.02)   \ndiscriminationTRUE     -2.51 ***      0.23 ***     -1.02 ***\n                       (0.09)        (0.03)        (0.03)   \noccupation                            1.80 ***      0.99 ***\n                                     (0.01)        (0.01)   \nability                                             2.01 ***\n                                                   (0.02)   \n------------------------------------------------------------\nR^2                     0.08          0.91          0.95    \nAdj. R^2                0.08          0.91          0.95    \nNum. obs.           10000         10000         10000       \nRMSE                    4.27          1.35          1.00    \n============================================================\n*** p < 0.001, ** p < 0.01, * p < 0.05\nWhen you’re interpreting these sort of tables, make sure you use the standard errors. The coefficients are estimates, so when making comparisons it is useful to look at the coefficients 95% confidence interval. The 95% confidence interval is just the estimate \\(\\pm\\) 2 * the standard error. If the actual value of the parameter is within the confidence interval for the parameter estimate, then we say that the estimate agrees with the actual parameter value.\nAll of the estimates in mod3 line up with the actual parameter values. This model correctly recovers the parameters, so we can be confident that this model is correct.\nBefore moving on, have a look at the coefficient estimates on the Discrimination term. It changes model to model, which makes sense since both mod1 and mod2 are incorrectly specified. Look at the coefficient in mod2 - it’s a different sign to the others! If you just used mod2 to make inferences, you’d conclude that discrimination increases wages. Bias can have huge effects on models, so it’s worth the effort to make sure your model is correct!"
  },
  {
    "objectID": "posts/rates_standardisation/index.html",
    "href": "posts/rates_standardisation/index.html",
    "title": "Rates & standardisation",
    "section": "",
    "text": "Which country do you think has a higher death rate - Kazakhstan or Sweden?\nKazakhstan seems like a reasonable guess, death rates in Kazakhstan are higher than Sweden for all ages:\nHere’s the data for the chart if you want to recreate it. If we calculate the crude death rate (total deaths / total population) it turns out that Sweden has the higher death rate, which feels slightly weird:\nThe reason why this is happening is because Sweden is much older than Kazakhstan. Old people tend to die more than young people, so a higher proportion of Sweden’s population die compared to Kazakhstan, which causes the higher rate. We can’t compare the crude death rates of the two countries because their populations are so different. We need a way to control for these differences. There’s two common techniques to do this in public health - direct standardisation and indirect standardisation."
  },
  {
    "objectID": "posts/rates_standardisation/index.html#direct-standardisation",
    "href": "posts/rates_standardisation/index.html#direct-standardisation",
    "title": "Rates & standardisation",
    "section": "Direct standardisation",
    "text": "Direct standardisation\nThe big idea behind standardisation is that every rate is a sum. Imagine you have data on the number of deaths & total population by ageband, you can make age-specific rates by dividing the number of deaths by the population in each age band. Then you could write the crude death rate as\n\\[\nCDR = \\frac{\\sum_{a}pop_{a}  rate_{a}}{\\sum_{a}pop_{a}} = \\sum_{a}c_{a}  rate_{a}\n\\]\nWhere \\(c\\) is the proportion of the population in each age group, which is usually called the population composition.\nSo the overall rate is made up of 2 things - the composition of the country, and the age-specific death rates of the country. If we replace the country composition with some other reference composition in the formula, we get directly standardised rates.\nIf you don’t have a reference composition to hand, a good choice is to take the average composition of the countries you’re comparing. So let’s make that:\n\ndf = df %>%\n  group_by(country) %>%\n  mutate(composition = pop / sum(pop))\n\nref_comp = df %>%\n  group_by(ageg) %>%\n  summarise(composition = mean(composition))\n\nNow just multiply these by the age-specific rates of each country and sum to get the DSRs:\n\nrates = df %>%\n  select(country, ageg, rate)\n  \nref_comp %>%\n  left_join(rates) %>%\n  group_by(country) %>%\n  summarise(dsr = sum(composition * rate))\n\n\n\n\n\n \n  \n    country \n    dsr \n  \n \n\n  \n    Kazakhstan \n    11.88 \n  \n  \n    Sweden \n    7.37 \n  \n\n\n\n\n\nKazakhstan has a higher death rate than Sweden, once you control for the age structure of the population. This feels a lot more reasonable, and lines up with what we saw in the chart at the start."
  },
  {
    "objectID": "posts/rates_standardisation/index.html#indirect-standardisation",
    "href": "posts/rates_standardisation/index.html#indirect-standardisation",
    "title": "Rates & standardisation",
    "section": "Indirect standardisation",
    "text": "Indirect standardisation\nSometimes you don’t have the composition & age-specific rates for each country, so you can’t do direct standardisation. If you’re only missing the age-specific rates (so you know the population & number of deaths, so you can calculate a CDR) you can do indirect standardisation.\nThe idea behind indirect standardisation is to use the country which you have all the information for as your reference area. By applying the reference country’s age-specific rates to the other country’s composition, you can estimate how many deaths you would expect in the other country assuming the country experienced the same death rates as the reference country. Comparing the actual & estimated CDRs gives you the standardised mortality ratio, which you then multiply the reference country’s CDR by to get the indirectly standardised rate.\nIf you’re into formulas the steps are:\n\nestimate the crude death rate for country you don’t have all the info on, by using the country’s composition & the reference areas rates:\n\n\\[CDR_{est} = \\sum_ac_a^{area}rate_a^{ref}\\]\n\nCompare this to the actual CDR to get the standardised mortality ratio:\n\n\\[SMR = \\frac{CDR_{actual}}{CDR_{est}}\\]\n\nUse the SMR to scale up the CDR for the reference area:\n\n\\[ISR = CDR^{ref} \\times SMR\\]\nImagine we didn’t have the age-specific rates for Kazakhstan, we can use Sweden as our reference area and compute the ISR. First we need to get the estimated crude death rate by applying Sweden’s rates to Kazakhstan’s composition:\n\nref_area = df %>%\n  filter(country == 'Sweden') %>%\n  select(country, ageg, rate)\n\nkaz = df %>%\n  filter(country == 'Kazakhstan') %>%\n  select(country, ageg, composition)\n\ncdr_est = ref_area %>%\n  left_join(kaz, by = 'ageg') %>%\n  summarise(cdr = sum(composition * rate))\n\nNext compare the estimated CDR to the actual Kazakhstan CDR we computed at the start:\n\nsmr = 7.42 / cdr_est$cdr\n\nFinally use this to scale up the Sweden CDR we computed at the start:\n\nisr_kaz = 10.55 * smr\n\nThis gives an indirectly standardised rate of 18.64 for Kazakhstan, which is fairly close to the directly standardised rate of 16.34 (just multiply Sweden’s composition by Kazakhstan’s rates)."
  },
  {
    "objectID": "posts/rates_standardisation/index.html#summary",
    "href": "posts/rates_standardisation/index.html#summary",
    "title": "Rates & standardisation",
    "section": "Summary",
    "text": "Summary\n\nDirect standardisation applies group rates from different areas to a reference composition. This lets you compare rates between areas with significantly different population structures\nDirect standardisation needs the population & number of events for each group, in all the areas you want to compare\nIf you’re missing the number of events in the groups for one area, you can do indirect standardisation\nIndirect standardisation applies the rates of one area to the composition of another, to estimate a rough crude death rate. From that you can compute the standardised mortality ratio. Multiplying the reference areas crude death rate by the SMR gives an indirectly standardised rate for the other area. The ISR is usually close enough to the DSR to be a good estimate"
  },
  {
    "objectID": "posts/rf_picture/index.html",
    "href": "posts/rf_picture/index.html",
    "title": "Random forests - in pictures",
    "section": "",
    "text": "But what do random forests actually do? You might have heard something about random forests being “an average of decision trees”, but what does that mean? This post will explain random forests. We’ll start off with decision trees and gradually work our way up to random forests. And - best of all - we’ll do it all with pictures! No maths, no code, just pretty pictures.\n\nThe Problem\nLook at this picture:\n\n\n\n\n\nIt’s a bunch of different coloured points. For the rest of this post, we’re going to try to predict the colour of a point, given its location.\nThis sounds like a daft made up problem, but it’s not too different to the sort of problems you’ll see in the ‘real world’. You have two dependent variables (the \\(x\\) and \\(y\\) coordinates of a point), and you want to predict a continuous variable (colour). It’s a regression problem.\nThere’s a fair bit of structure to this data:\n\nthere are 4 bright horizontal & vertical lines, and they sort of look like a hashtag\nthere are 4 bright spots where the bright lines overlap\n\nA good model should be able to capture this structure.\nInstead of just jumping straight to random forests, we’re going to start with a slightly simpler model - decision trees.\n\n\nDecision Trees\nA decision tree is a sequence of yes/no questions. Each of these yes/no questions ‘split’ the dataset into smaller and smaller chunks. The idea behind decision trees is that, by splitting the data down into small groups, each group will contain data points which are fairly similar to each other. Since the points in each group are fairly similar to each other, the average value in each group will be fairly close to the actual value of each point. This is how predictions are made using decision trees - figure out which group a new data point is in, then predict the average value for that group.\nHere’s how our data looks to a simple decision tree:\n\n\n\n\n\nClearly, this isn’t the best model in the world. It hasn’t picked up any of the patterns in the data and only ever predicts two values - the model thinks any point on the far left is purple, and all other points are yellow. This is because we’ve fit a very shallow decision tree. The tree is only allowed to make one split, so the best it’ll ever be able to do is split the data into two smaller chunks. If we let the tree make more splits it’ll be able to make smaller chunks of the data, so it should be able to pick up more of the underlying patterns in the data.\nHere’s how our data looks to a tree of depth 3:\n\n\n\n\n\nThis is slightly better. It’s picked up the bright band on the right hand side, and it’s figured out that the bottom right corner is a darker colour than the other areas. There’s still a lot of patterns which the model hasn’t picked up however.\nHere’s how the data looks to a tree of depth 5:\n\n\n\n\n\nNow we’re getting somewhere! The model has picked up the two bright vertical bands, has figured out that there is a very bright spot on the left, and has noticed that points in the corners are darker than the rest of the points.\nIncreasing the tree depth increases the number of splits. This means the tree is able to split the dataset into smaller regions. This increased flexability lets the model find more complex patterns in the data.\nYou might be temped to keep on increasing the depth of the tree, since it looks like deeper trees make better predictions. This is true, but you need to be careful that you don’t start overfitting your data. Imagine if you made an incredibly deep tree, so deep that each data point was in it’s own group. The predictions from the tree would be absolutely perfect since each point is in it’s own group, and so the average value for the group - the predicition - will be exactly the same as the actual value of the point. But this model hasn’t actually found any patterns in the data, it’s just memorised the value for each datapoint! The goal of machine learning is to figure out patterns in data, then use those patterns to make predictions about new data.\nOne particularly nice feature about decision trees is what questions they ask when splitting data. The questions are always things like ‘is x less than 50?’ or ‘is z bigger than 264?’. This is really nice because it means that decision trees don’t care about outliers. They don’t care about the actual value of a datapoint, only if that value is above/below some threshold value.\n\n\nRandom Forests\nThere’s only one downside to decision trees - the predictions they make aren’t amazing. The predictions are quite accurate, but not as accurate as some of the other fancy machine learning models out there. Fortunately there’s a trick we can do to get much more accurate predictions out of decision trees - ensembling.\nThe idea behind ensembling is really simple. Imagine you have loads of different models. You can make predictions with each of these models, giving you a bunch of predictions. Each of these predictions will have an error associated with them. If these errors are\n\nindependent - that is, if knowing something about the error of one prediction doesn’t tell you anything about the error of another prediction, and\nrandom - that is, the errors of each model predictions aren’t systematically large/small for particular groups of data\n\nThen averaging the predictions will give you a more accurate prediction.\nSo we need a bunch of models, which are all independent from each other. Can we do something to our data to end up with loads of independent decision trees? If we can, then we can just average the predictions from each of these trees and get a much better model.\nRemember what decision trees do - they split the dataset into smaller regions, then make predictions based on which region a point falls into. The splits depend on the underlying patterns in the dataset. If the dataset changes, then the small regions will also change. If we were to take a random sample of the data, we would get a different decision tree.\nLet’s give that a go. Here’s two decision trees, each fit to a different random sample of the data:\n\n\n\n\n\nThey look like very different models, so we can ensemble them. Here’s the average of the predictions from the two models:\n\n\n\n\n\nThat looks very similar to the original data! It’s picked up the bright lines & bright spots, so it’s managed to find the structure in the data.\nThis average model is a random forest. Here’s the steps for building a random forest:\n\nGet some data\nTake random samples of the data\nBuild a decision tree on each of the random samples\nMake predictions with each of the decision trees\nAverage the predictions\n\n\n\nImproving The Forest\nNow that we’ve built a random forest, can we make it better? Broadly, there are three ways of improving the performance of an ensemble model:\n\nIncrease the number of models in the ensemble. More models gives you more opportunities to spot patterns in the data\nImprove the performance of the underlying model. For us, that’d mean improving the decision trees (by increasing the depth for example)\nMake the models more uncorrelated with each other. If the models are more uncorrelated with each other, then they’re going to be more independent, so the ensemble should be more accurate\n\nRandom forests do all sorts of tricks to make the models more uncorrelated with each other. The best trick (and one you need to know about) is that the forest isn’t allowed to split on the same variables at each split. Every time the model needs to make a split, it randomly chooses a handful of features to split on. This reduces the chances of several trees splitting on the same feature at the same point, which makes the trees less similar to each other.\nAll random forest algorithms let you decide how many features you want to consider at each split. There’s a couple of default values which are good to try out if you’re not sure - for classification problems, it’s best to try using \\(\\sqrt{n_{features}}\\). For regression, try out \\(n_{features}\\).\n\n\nThese are just rough rule-of-thumb defaults. A better idea would be to try grid search, and an even better idea would be to ask a subject matter expert!\nThere’s only two features in our dataset (the x & y coordinates), so it doesn’t look like there’s much we can do to make the trees more uncorrelated with each other. Let’s try out the other two bulletpoints.\nHere’s a random forest model with 1000 trees. Each tree has depth 15, so can split the dataset up into lots of little rectangular regions:\n\n\n\n\n\nLooks good! This has found the bright lines & spots (same as before), but now it’s figured out that the spots are all the same colour. Also, the spots look like actual spots. They’re round! It’s pretty amazing that round regions have appeared, considering that the decision trees in the ensemble can only ever make rectangular regions. Given enough flexibility (tree depth), random forests are able to fit arbitrarily complex patterns in data.\n\n\nWrap Up\nIn this post we’ve seen:\n\nDecision trees split datasets up into lots of little rectanglular regions\nDecision trees make predictions by figuring out which rectangular region a new data point is in, then predicting the average value of that region\nAveraging predictions will give you a better prediction, provided that the predictions are independent from each other and have random errors. This is called ensembling\nYou can easily make lots of uncorrelated models by training decision trees on different samples of data\nAveraging these decision trees makes a random forest\nEnsembles can be improved by increasing the number of models in the ensemble, increasing the performance of the models in the ensemble, or by making the models less correlated with each other\n\nThat’s a lot of stuff! If you want to learn more about random forests, have a read over the original paper which introduced the idea. If you like the idea of ensembles, have a look at this RSS talk bag of little bootstraps by Michael Jordan. If you want to see another random forest algorithm which introduces even more randomness into the decision trees, check out extremly randomised trees.\nFinally, if you’re interested in the code used to make these models & graphs, my workings are in this notebook."
  },
  {
    "objectID": "posts/simulating_power_analysis/index.html",
    "href": "posts/simulating_power_analysis/index.html",
    "title": "Power analysis by simulation",
    "section": "",
    "text": "The alternative to fancy formulas & tools is to simulate your experiment. The rough idea is you simulate your experiment a bunch of times, then check to see how often you detect an effect. This reduces the amount of maths and increases the types of designs you can do power analysis with, but it increases the amount of coding you need to do - which feels like a good trade!\n\nWhy do power analysis\nPower analysis is part of experimental design - the stuff you do before running an experiment. Very broadly, there’s a bunch of things you might be interested in when you do an experiment:\n\nThe size of an effect (if you do a treatment X, how much does an outcome Y change?)\nThe variation in treatment X\nThe left over variation in Y (so something like \\(R^2\\) from a model \\(Y \\sim X\\))\nStatistical precision (if you run this experiment, how likely are you to detect an effect?)\nThe sample size needed before you run an experiment\n\nIf you fix 4 of these 5 things, power analysis will tell you about the fifth thing. For example if you are doing an experiment on 100 people, you already know the variation in X & Y, and you want at least a 80% chance of detecting an effect, power analysis will tell you the minimum detectable effect - the smallest effect size you’ll be able to measure with the experiment.\nOr imagine if you have a rough idea of the effect size (for example from past experience or lit reviews), you know the variation in X & Y, and you want an 80% chance of detecting an effect, then power analysis will tell you the smallest sample size required.\nThis is why power analysis is such an important part of the design - if the analysis says you need at least a sample of 1,000 people but you only have 100, there’s not much point in doing the experiment!\n\n\nSimulating\nThe idea behind simulating is quite straightforward - you just simulate the experiment over & over, and see how many times you’re able to detect an effect. Here’s an example:\n\nI think \\(Y\\) is related to \\(X\\), but I don’t know how large the effect is. \\(X\\) is uniformly distributed between 0 & 1 in the population.\nI’ve got a sample of 1,000 people and - if an effect exists - I want to correctly detect it at least 80% of the time. How large does the effect size need to be?\n\nAll we’re going to do is simulate the data over and over, fit a linear model & see if the \\(X\\) coefficient is significant. First of all lets make a function to simulate the experiment:\n\nexperiment_sim = function(effect, sample_size){\n\n  # Simulate experimental data\n  X = runif(sample_size, 0, 1) # X is uniformly distributed between 0 & 1\n  Y = effect * X + rnorm(sample_size, mean = 0, sd = 3) # Y is related to X\n\n  # Run the analysis & get it into a tidy format\n  mod = lm(Y ~ X)\n  mod = broom::tidy(mod)\n  \n  # Check if the X term is significant\n  sig = mod$p.value[[2]] < 0.05\n  return(sig)\n}\n\nFor any given effect size, we can use this function to figure out how many times the experiment would detect the effect. For example, if the effect size was 0.2, then the experiment would detect the result about 12.6% of the time:\n\n# Run the experiment 500 times with an effect size of 0.2\nset.seed(19532)\n\nn_experiment = 500\nsig = vector('logical', n_experiment)\nfor (i in 1:n_experiment){\n  sig[[i]] = experiment_sim(0.2, 1000)\n}\n\nmean(sig)\n\nSo to figure out how large the effect size needs to be in order for the experiment to detect it 80% of the time, we just have to try a bunch of different effect sizes (I’ve reduced the number of experiments from 500 to 100 to speed things up):\n\nset.seed(65439)\n\neffect_sizes = seq(0.2, 2, by = 0.2)\npct_detect = vector('double', length(effect_sizes))\nn_experiment = 100\n\nfor (j in seq_along(effect_sizes)){\n  # Repeatedly run the experiment with the effect size & count the number of times\n  # the experiment would detect the effect\n  \n  sig = vector('logical', n_experiment)\n  for (i in 1:n_experiment){\n    sig[[i]] = experiment_sim(effect_sizes[[j]], 1000)\n  }\n  \n  pct_detect[[j]] = mean(sig)\n  \n}\n\n\n\n\n\n\nThis experiment would be able to consistently detect effect sizes around 1 or larger. If you don’t think the effect is going to be that large, don’t do the experiment! You’ll need to improve the design somehow - increase the sample size, reduce variation in X or Y, use more accurate analysis steps, …\nIf someone asks a different question, it’s very easy to flip the code around to answer it. For example imagine if someone asks you about sample sizes:\n\nI think \\(Y\\) is related to \\(X\\), I’ve read loads of papers and I think the effect size is probably 0.3ish. \\(X\\) is uniformly distributed between 0 & 1 in the population.\nI want to do an experiment to figure out the effect size, how many people do I need to include in my experiment?\n\nHere’s the code to answer the question - look how similar it is! In the last bit of code we fixed the sample size & varied the effect sizes, now we fix the effect size & vary the sample sizes:\n\nset.seed(65439)\n\nsample_sizes = 10^seq(1, 5, by = 0.25)\npct_detect = vector('double', length(sample_sizes))\nn_experiment = 100\n\nfor (j in seq_along(sample_sizes)){\n  # Repeatedly run the experiment with the effect size & count the number of times\n  # the experiment would detect the effect\n  \n  sig = vector('logical', n_experiment)\n  for (i in 1:n_experiment){\n    sig[[i]] = experiment_sim(0.3, sample_sizes[[j]])\n  }\n  \n  pct_detect[[j]] = mean(sig)\n  \n}\n\n\n\n\n\n\n\n\nA more complex example\nThe examples so far haven’t been massively difficult, they’re the sort of problems you can use calculators like G*Power on. The cool thing about simulations is you can use them to analyse any sort of experimental setup, no matter how weird looking it is.\nHere’s a bit of a tricky example:\n\nYou’ve got 1,000 people who are split into 10 equally sized groups. Treatment is assigned at the group level (so either everyone in a group gets the treatment, or no one in the group gets the treatment). Each person has a \\(W \\sim \\text{Unif}(0, 1)\\), and treatment is given based on the average \\(W\\) within the group.\nYou think the true effect is somewhere around 0.5, how big of a sample do you need to be able to detect the effect 80% of the time?\n\nThere’s 2 weird things going on here - most tools for power analysis assume treatment is done at the person level, but here the treatment is assigned at group level. Also there’s a confounder \\(W\\) we need to control for in our analysis, we can’t just do a simple regression of \\(Y \\sim X\\). Fortunately none of this matters for simulation! We just follow the same process as before: Simulate the experiment a bunch of times with a bunch of different sample sizes and see how often the effect is detected:\n\nexperiment_sim = function(effect, sample_size){\n  \n  ### Generate the data - need to be careful with the group treatment\n  df = data.frame(W = runif(sample_size),\n                  group = sample(1:10, sample_size, replace = T))\n  \n  # Figure out which groups get treated. Say each group gets treated\n  # with probability 0.5 + mean(W) / 10. We can check this using\n  # the fact that X > runif() occurs with probability X, for X between 0 & 1\n  df_treat = df %>%\n    group_by(group) %>%\n    summarise(mean_w = mean(W)) %>%\n    mutate(treated = 0.5 + mean_w / 10 > runif(nrow(.)))\n  \n  # Treatment depends on the group you're in, and your value of W\n  df = df %>%\n    left_join(df_treat, by = 'group') %>%\n    mutate(Y = effect * treated + W + group/10 + rnorm(sample_size))\n  \n  ### Do the analysis\n  mod = lm(Y ~ treated + W, data = df)\n  mod = broom::tidy(mod)\n  \n  sig = mod$p.value[[2]] < 0.05\n  return(sig)\n}\n\n# Run the experiment a bunch of times, varying the sample size\n# Exactly the same code as above\nset.seed(592)\n\nn_experiment = 500\nsample_sizes = seq(50, 500, by = 50)\npct_detect = vector('double', length(sample_sizes))\n\nfor (j in seq_along(sample_sizes)){\n  sig = vector('logical', n_experiment)\n  for (i in 1:n_experiment){\n    sig[[i]] = experiment_sim(0.5, sample_sizes[[j]])\n  }\n  pct_detect[[j]] = mean(sig, na.rm = T)\n}\n\n\n\n\n\n\nTurns out that an effect size of 1 in this setup is very large, so we don’t need a lot of data to be able to detect the effect.\n\n\nConclusion\nSo that’s it! Using simulation, there’s only 2 steps to any power analysis:\n\nWrite a function to simulate your experiment - the function needs to generate the data from the experiment, and run the analysis on the simulated data\nRun the function lots of times, varying the parameter you’re interested in"
  }
]